import sqlite3
import numpy as np
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
from typing import List, Dict, Optional, Tuple
import faiss
from tqdm import tqdm
import warnings
from datetime import datetime
import os
import time
import logging
import re # Import re for regular expression operations

# --- Configuration ---
# You might consider moving these to a separate config.py or .env file for easier management
#MODEL_PATH = "openthaigpt-1.0.0-7b-chat.Q4_K_M.gguf"
MODEL_PATH = "Q4_KM.gguf"

DB_PATH = "rag_data.sqlite"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
TOP_K_RESULTS = 5 # Number of top results to retrieve from FAISS initially
EMBEDDING_DIMENSION = 768 # Dimension of the embeddings generated by EMBEDDING_MODEL

# FAISS Index persistence paths
FAISS_INDEX_FILE = "faiss_index.bin"
FAISS_IDS_FILE = "faiss_ids.npy"

# --- Text Preprocessing Constants ---
MIN_TEXT_LENGTH = 10  # Minimum length of text to be considered valid for embedding
MAX_TEXT_LENGTH = 2000 # Maximum length of text for embedding (adjust based on model's context window)

# --- Retrieval Optimization Constants ---
CONTEXT_EXPANSION_WINDOW = 0 # Number of additional chunks to fetch before and after a relevant chunk
                               # E.g., 2 means fetch 2 previous and 2 next chunks.

class BuddhistRAGSystem:
    def __init__(self, db_path: str = DB_PATH):
        """
        Enhanced RAG system for Buddhist scriptures with auto-setup.
        Initializes logging, verifies model and database resources,
        and sets up the LLM, embedding model, and FAISS index.
        """
        self.db_path = db_path
        self.error_log = []
        self.faiss_index = None # Stores the FAISS index object
        self.faiss_ids = None   # Stores the array of IDs corresponding to FAISS index entries
        self._setup_logging() # Initialize logging first to capture all events
        try:
            self._verify_model_file()
            self._verify_resources()
            self._init_resources()
        except Exception as e:
            self.error_log.append(str(e))
            self.log_error(f"Initialization failed: {str(e)}")
            self._handle_critical_error(e)
            print("- ‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤ (.gguf) ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞...")
            raise # Re-raise the exception to stop execution if critical init fails

    def _setup_logging(self):
        """Sets up the logging configuration for the system."""
        self.start_time = datetime.now()
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            filename='buddha_rag.log',
            filemode='a' # Append to the log file
        )
        print(f"\nü™∑ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ô‡∏≤ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')})")
        print("=" * 60)

    def log_error(self, message: str):
        """Logs an error message to the console and file."""
        logging.error(message)
        self.error_log.append(message)

    def log_warning(self, message: str):
        """Logs a warning message to the console and file."""
        logging.warning(message)

    def _handle_critical_error(self, e):
        """
        Placeholder for handling critical errors during system initialization.
        In a production environment, this might involve more robust error reporting or shutdown procedures.
        """
        print(f"A critical error occurred during initialization: {e}")
        # Consider adding sys.exit(1) here for unrecoverable errors.

    def _verify_model_file(self):
        """Verifies the existence and integrity of the language model file."""
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"Model file not found: {MODEL_PATH}")

        file_size = os.path.getsize(MODEL_PATH) / (1024 * 1024 * 1024)
        if file_size < 3.5: # A heuristic check for incomplete downloads (e.g., 7B model is ~3.8GB)
            raise ValueError(f"Incomplete model file (size: {file_size:.2f}GB, expected ~3.8GB)")
        logging.info(f"Model file verified: {MODEL_PATH} (Size: {file_size:.2f}GB)")

    def _verify_resources(self):
        """
        Verifies the existence and validity of all required resources,
        including the model file and the SQLite database.
        If the database is corrupted or missing, it attempts to recreate it.
        """
        # Model file check is already done in _verify_model_file
        
        if os.path.exists(self.db_path):
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                # Simple check to see if the database is accessible
                cursor.execute("SELECT 1 FROM sqlite_master LIMIT 1")
                cursor.fetchone()
                # Perform an integrity check
                cursor.execute("PRAGMA integrity_check;")
                integrity_results = cursor.fetchall()
                if integrity_results != [('ok',)]:
                    logging.error(f"‚ö†Ô∏è ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢: {integrity_results}")
                    conn.close()
                    raise sqlite3.Error("Database integrity check failed")
                conn.close()
                logging.info("Database connection verified and integrity checked.")
            except sqlite3.Error as e:
                self.log_error(f"‚ö†Ô∏è ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {str(e)}")
                print("‚ö†Ô∏è ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà...")
                # Consider backing up the old database before removing
                if os.path.exists(self.db_path):
                    os.remove(self.db_path)
                self._create_database() # Recreate the database
                logging.info("Database recreated.")
        else:
            logging.info("Database file not found, creating new one.")
            self._create_database() # Create the database if it doesn't exist
        logging.info("Resources verified.")

    def _init_resources(self):
        """Initializes the LLM, Embedding Model, and FAISS Index."""
        self._init_embedding_model()
        self._init_llm()
        self._init_database()
        # _load_faiss_index returns a tuple (index, ids_array), so self.faiss_index and self.faiss_ids store them
        self.faiss_index, self.faiss_ids = self._load_faiss_index()
        print("\n‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß!")
        logging.info("System initialized successfully.")

    def _init_embedding_model(self):
        """Initializes the sentence embedding model."""
        try:
            self.embedding_model = SentenceTransformer(
                EMBEDDING_MODEL,
                device='cpu', # Use 'cuda' if you have a compatible GPU, 'cpu' otherwise
                cache_folder='./model_cache' # Cache downloaded models to avoid re-downloading
            )
            self._verify_embedding_model() # Verify the model is working correctly
            logging.info("Embedding model initialized.")
        except Exception as e:
            self.log_error(f"Failed to initialize embedding model: {str(e)}")
            raise

    def _verify_embedding_model(self):
        """Verifies the embedding model by encoding a test text and checking its dimension."""
        test_text = "‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ô‡∏≤"
        embedding = self.embedding_model.encode([test_text])
        if len(embedding[0]) != EMBEDDING_DIMENSION:
            raise ValueError(
                f"Embedding model output dimension mismatch (expected: {EMBEDDING_DIMENSION}, got: {len(embedding[0])})")
        logging.info(f"Embedding Model Ready (Dimension: {EMBEDDING_DIMENSION})")
        print(f"‚úÖ Embedding Model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏°‡∏¥‡∏ï‡∏¥: {EMBEDDING_DIMENSION})")

    def _init_llm(self):
        """Initializes the Large Language Model (LLM) using llama_cpp."""
        print("\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤ (‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ô‡∏≤‡∏ó‡∏µ)...")
        logging.info("Initializing LLM...")
        try:
            self.llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=4096,    # Context window size for the LLM
                n_threads=6,   # Number of CPU threads to use for LLM inference
                n_batch=512,   # Batch size for prompt processing by LLM
                n_gpu_layers=0, # Number of GPU layers to offload (0 means CPU only)
                verbose=False, # Suppress verbose output from llama_cpp
                logits_all=False # Only compute logits for the last token for efficiency
            )
            self._test_llm() # Run a quick test to ensure LLM is functional
            logging.info("LLM initialized.")
        except Exception as e:
            self.log_error(f"Failed to initialize LLM: {str(e)}")
            raise

    def _test_llm(self):
        """Performs a simple test to ensure the LLM is loaded and responsive."""
        try:
            test_response = self.llm("Hello, how are you?", max_tokens=10, temperature=0.1, stop=["\n"])
            if not test_response or 'choices' not in test_response or len(test_response['choices']) == 0:
                raise ValueError("LLM test response is empty or malformed.")
            logging.info(f"LLM test successful. Sample response: {test_response['choices'][0]['text'].strip()}")
            print("‚úÖ LLM ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        except Exception as e:
            self.log_error(f"LLM self-test failed: {e}")
            raise RuntimeError(f"LLM self-test failed: {e}")

    def _init_database(self):
        """Initializes the SQLite database connection and sets PRAGMA options."""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.execute("PRAGMA journal_mode=WAL") # Write-Ahead Logging for better concurrency
            self.conn.execute("PRAGMA synchronous=NORMAL") # Balance safety and performance
            self._create_tables() # Ensure tables exist
            logging.info(f"Database initialized at: {self.db_path}")
            print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {self.db_path}")
        except sqlite3.Error as e:
            self.log_error(f"Database error: {str(e)}")
            raise

    def _create_database(self):
        """Creates a new SQLite database file and initializes its tables."""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            cursor = conn.cursor()
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS texts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    vol INTEGER NOT NULL,
                    page INTEGER NOT NULL,
                    item INTEGER NOT NULL,
                    content TEXT NOT NULL,
                    embedding BLOB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(vol, page, item)
                )
            """)
            conn.commit()
            conn.close()
            logging.info(f"New database created and tables initialized at: {self.db_path}")
        except sqlite3.Error as e:
            self.log_error(f"Error creating new database: {str(e)}")
            raise

    def _create_tables(self):
        """Creates the 'texts' table if it doesn't already exist in the connected database."""
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS texts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vol INTEGER NOT NULL,
                page INTEGER NOT NULL,
                item INTEGER NOT NULL,
                content TEXT NOT NULL,
                embedding BLOB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(vol, page, item)
            )
        """)
        self.conn.commit()
        logging.info("Database table 'texts' created or exists.")

    def _load_faiss_index(self, rebuild_if_corrupted=False) -> Tuple[Optional[faiss.Index], Optional[np.ndarray]]:
        """
        Loads the FAISS index and corresponding IDs from disk if available and valid.
        Otherwise, it loads from the database and rebuilds the index.
        Offers an option to force rebuild if it's corrupted or problematic.
        """
        print("\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î FAISS index...")
        # Try to load FAISS index from disk first for faster startup
        if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(FAISS_IDS_FILE) and not rebuild_if_corrupted:
            try:
                index = faiss.read_index(FAISS_INDEX_FILE)
                ids = np.load(FAISS_IDS_FILE)
                # Basic validation: check dimension and number of vectors
                if index.d == EMBEDDING_DIMENSION and index.ntotal == len(ids):
                    logging.info(f"FAISS index loaded from file: {FAISS_INDEX_FILE}")
                    print("‚úÖ FAISS index ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à.")
                    return index, ids
                else:
                    self.log_warning("Loaded FAISS index from file has dimension or count mismatch. Will rebuild.")
            except Exception as e:
                self.log_error(f"Error loading FAISS index from file: {e}. Will rebuild from database.")
        
        # If loading from file fails or not present, proceed to load from DB and rebuild
        logging.info("Loading FAISS index from database and rebuilding.")
        index, ids = self._load_index_data_from_db() # This handles generating missing embeddings too
        
        if index is not None and ids is not None:
            # Save the newly built index to disk for future fast loading
            try:
                faiss.write_index(index, FAISS_INDEX_FILE)
                np.save(FAISS_IDS_FILE, ids)
                logging.info(f"FAISS index saved to disk: {FAISS_INDEX_FILE}, {FAISS_IDS_FILE}")
            except Exception as e:
                self.log_error(f"Failed to save FAISS index to disk: {e}")
            logging.info("FAISS index loaded and rebuilt successfully.")
            print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(ids):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
            return index, ids
        else:
            self.log_error("Failed to load or rebuild FAISS index from database.")
            print("‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à.")
            return None, None

    def rebuild_faiss_index(self):
        """
        Rebuilds the FAISS index from all embeddings stored in the database.
        This is useful if the index becomes corrupted or new data is added.
        """
        logging.info("Rebuilding FAISS index (forced).")
        # Clear existing index references to ensure a fresh rebuild
        if hasattr(self, 'faiss_index') and self.faiss_index is not None:
            del self.faiss_index
            self.faiss_index = None
        if hasattr(self, 'faiss_ids') and self.faiss_ids is not None:
            del self.faiss_ids
            self.faiss_ids = None

        # Call _load_faiss_index with rebuild_if_corrupted=True to force loading from DB
        self.faiss_index, self.faiss_ids = self._load_faiss_index(rebuild_if_corrupted=True)
        if self.faiss_index is not None and self.faiss_ids is not None:
            logging.info("FAISS index rebuilt successfully.")
            print("‚úÖ FAISS index ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à.")
        else:
            self.log_error("Failed to rebuild FAISS index.")
            print("‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à.")

    def _load_index_data_from_db(self) -> Tuple[Optional[faiss.Index], Optional[np.ndarray]]:
        """
        Helper function to fetch embeddings from the database and populate the FAISS index.
        Prompts the user to generate/regenerate embeddings if missing or dimension mismatch occurs.
        """
        cursor = self.conn.cursor()

        # Check if any embeddings exist
        cursor.execute("SELECT COUNT(*) FROM texts WHERE embedding IS NOT NULL")
        if cursor.fetchone()[0] == 0:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Embedding ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            logging.warning("No embeddings found in database.")
            if self._confirm_action("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?"):
                self._generate_missing_embeddings()
                # Recursively call to load index after generating embeddings
                return self._load_index_data_from_db()
            return None, None

        # Verify embedding dimension consistency
        cursor.execute("SELECT embedding FROM texts WHERE embedding IS NOT NULL LIMIT 1")
        sample = cursor.fetchone()
        dimension = len(np.frombuffer(sample[0], dtype=np.float32)) if sample and sample[0] else 0
        if dimension != EMBEDDING_DIMENSION:
            print(f"‚ö†Ô∏è ‡∏°‡∏¥‡∏ï‡∏¥ Embedding ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á ({dimension} != {EMBEDDING_DIMENSION})")
            logging.error(f"Embedding dimension mismatch: expected {EMBEDDING_DIMENSION}, got {dimension}")
            if self._confirm_action("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?"):
                self._regenerate_all_embeddings()
                # Recursively call to load index after regenerating embeddings
                return self._load_index_data_from_db()
            return None, None

        # Initialize FAISS index
        # Using IndexFlatL2 for simplicity, consider IndexIVFFlat or IndexHNSW for larger datasets
        index = faiss.IndexFlatL2(dimension) # L2 distance (Euclidean distance)
        index = faiss.IndexIDMap(index)     # Map internal FAISS IDs to external database IDs
        ids = []

        batch_size = 1000
        total_embeddings_count = self._get_embedding_count()
        total_batches = (total_embeddings_count + batch_size - 1) // batch_size

        # Load embeddings in batches to avoid high memory usage
        for batch_num in tqdm(range(total_batches), desc="Loading Embeddings into FAISS"):
            offset = batch_num * batch_size
            cursor.execute(
                "SELECT id, embedding FROM texts WHERE embedding IS NOT NULL LIMIT ? OFFSET ?",
                (batch_size, offset)
            )

            batch_ids = []
            batch_embeddings = []

            for id, embedding_data in cursor.fetchall():
                try:
                    embedding = np.frombuffer(embedding_data, dtype=np.float32)
                    if len(embedding) == dimension:
                        batch_ids.append(id)
                        batch_embeddings.append(embedding)
                    else:
                        logging.error(f"Embedding dimension mismatch for ID {id} during load: {len(embedding)} != {dimension}")
                except Exception as e:
                    logging.error(f"Error processing embedding for ID {id} during load: {str(e)}")

            if batch_embeddings:
                emb_arr = np.stack(batch_embeddings).astype('float32')
                id_arr = np.array(batch_ids, dtype='int64')
                index.add_with_ids(emb_arr, id_arr)
                ids.extend(batch_ids)

        logging.info(f"FAISS index created with {len(ids)} entries.")
        return index, np.array(ids)

    def _get_embedding_count(self) -> int:
        """Helper function to get the total count of texts with embeddings in the database."""
        cursor = self.conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM texts WHERE embedding IS NOT NULL")
        return cursor.fetchone()[0]

    def _get_max_text_id(self, cursor) -> int:
        """Helper function to get the maximum text ID from the database."""
        cursor.execute("SELECT id FROM texts ORDER BY id DESC LIMIT 1")
        result = cursor.fetchone()
        return result[0] if result else 0

    def _clean_text(self, text: str) -> str:
        """
        Cleans and normalizes text for embedding.
        - Removes HTML tags, URLs, and emojis.
        - Removes specific metadata patterns like "‡πÄ‡∏•‡πà‡∏° X ‡∏´‡∏ô‡πâ‡∏≤ Y ‡∏Ç‡πâ‡∏≠ Z".
        - Replaces multiple whitespaces (including newlines, tabs) with a single space.
        - Strips leading/trailing whitespace.
        """
        if not isinstance(text, str):
            self.log_warning(f"Attempted to clean non-string input: {type(text)}. Converting to string.")
            text = str(text)
        
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Remove URLs
        text = re.sub(r'http\S+|www\S+', '', text)
        # Remove emojis (basic regex, might need more comprehensive patterns for all emojis)
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE
        )
        text = emoji_pattern.sub(r'', text)
        
        # NEW: Remove specific metadata patterns like "‡πÄ‡∏•‡πà‡∏° X ‡∏´‡∏ô‡πâ‡∏≤ Y ‡∏Ç‡πâ‡∏≠ Z"
        # This pattern needs to be carefully crafted based on your actual data format.
        # Example: "‡πÄ‡∏•‡πà‡∏° 19 ‡∏´‡∏ô‡πâ‡∏≤ 430 ‡∏Ç‡πâ‡∏≠ 1704 1705"
        text = re.sub(r'‡πÄ‡∏•‡πà‡∏°\s*\d+\s*‡∏´‡∏ô‡πâ‡∏≤\s*\d+\s*‡∏Ç‡πâ‡∏≠\s*\d+(\s*\d+)*', '', text)
        # Also remove square bracket numbers like [‡πë‡πó‡πê‡πî] which are often item numbers
        text = re.sub(r'\[\s*\d+\s*\]', '', text)
        text = re.sub(r'\[\s*\u0E50-\u0E59\s*\]', '', text) # Thai numerals in brackets

        # Replace multiple whitespaces (including newlines and tabs) with a single space
        text = re.sub(r'\s+', ' ', text)
        # Strip leading/trailing whitespace
        text = text.strip()
        
        return text

    def _is_valid_text(self, text: str) -> bool:
        """
        Checks if the cleaned text is suitable for embedding based on length criteria.
        """
        if not text or not isinstance(text, str):
            return False
        text = text.strip()
        if len(text) < MIN_TEXT_LENGTH:
            self.log_warning(f"Text too short for embedding (length {len(text)} < {MIN_TEXT_LENGTH}): '{text}'")
            return False
        if len(text) > MAX_TEXT_LENGTH:
            self.log_warning(f"Text too long for embedding (length {len(text)} > {MAX_TEXT_LENGTH}): '{text[:100]}...'")
            return False
        return True

    def _generate_missing_embeddings(self, batch_size: int = 100):
        """
        Generates embeddings for texts in the database that currently do not have them.
        The content is cleaned and validated before embedding generation.
        """
        cursor = self.conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM texts WHERE embedding IS NULL")
        missing_count = cursor.fetchone()[0]

        if missing_count == 0:
            print("‚úÖ ‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ Embedding ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
            logging.info("All texts have embeddings.")
            return

        print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {missing_count:,} ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
        logging.info(f"Generating embeddings for {missing_count} texts.")

        for offset in tqdm(range(0, missing_count, batch_size), desc="Generating embeddings"):
            cursor.execute("""
                SELECT id, content FROM texts
                WHERE embedding IS NULL
                LIMIT ? OFFSET ?
            """, (batch_size, offset))

            batch_updates = [] # (embedding_bytes, id)
            for id, content in cursor.fetchall():
                try:
                    cleaned_content = self._clean_text(content)
                    if not self._is_valid_text(cleaned_content):
                        # Logged by _is_valid_text, skip embedding for invalid content
                        continue
                        
                    # Ensure normalize_embeddings=True for content embeddings
                    embedding = self.embedding_model.encode([cleaned_content], convert_to_numpy=True, normalize_embeddings=True)[0]
                    if embedding.shape[0] != EMBEDDING_DIMENSION:
                        self.log_error(f"Generated embedding dimension mismatch for ID {id}: expected {EMBEDDING_DIMENSION}, got {embedding.shape[0]}")
                        continue

                    batch_updates.append((
                        embedding.astype(np.float32).tobytes(),
                        id
                    ))
                except Exception as e:
                    self.log_error(f"Error generating embedding for texts ID {id}: {str(e)}")

            if not batch_updates: # Skip if no valid data in batch
                continue

            try:
                self.conn.executemany(
                    "UPDATE texts SET embedding = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
                    batch_updates
                )
                self.conn.commit()
                logging.info(f"Processed {len(batch_updates)} embeddings in this batch. Total processed: {min(offset + batch_size, missing_count)}.")
            except sqlite3.Error as e:
                self.log_error(f"Error updating embeddings in database: {str(e)}")
                print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï embedding: {e}")
                # Do not return, try to continue with next batch
            print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß {min(offset + batch_size, missing_count):,}/{missing_count:,} ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")

    def _preprocess_query(self, query: str) -> str:
        """
        Preprocesses the input query by cleaning it.
        This ensures consistency between query embeddings and stored text embeddings.
        """
        return self._clean_text(query)

    def _regenerate_all_embeddings(self, batch_size: int = 50): # Adjusted default batch size for regeneration
        """
        Regenerates all embeddings in the database.
        This is useful if the embedding model changes or if the cleaning process is updated.
        """
        cursor = self.conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM texts")
        total_count = cursor.fetchone()[0]

        if total_count == 0:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            logging.warning("No data in the database to regenerate embeddings.")
            return

        if not self._confirm_action(f"‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_count:,} ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏•‡πâ‡∏≤‡∏á Embedding ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"):
            return

        # First, set all embeddings to NULL to mark them for regeneration
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡πâ‡∏≤‡∏á Embedding ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")
        logging.info("Clearing all existing embeddings for regeneration.")
        try:
            cursor.execute("UPDATE texts SET embedding = NULL, updated_at = CURRENT_TIMESTAMP")
            self.conn.commit()
            logging.info("All existing embeddings cleared.")
            # Also remove FAISS index files as they are now outdated
            if os.path.exists(FAISS_INDEX_FILE):
                os.remove(FAISS_INDEX_FILE)
            if os.path.exists(FAISS_IDS_FILE):
                os.remove(FAISS_IDS_FILE)
            logging.info("FAISS index files removed.")
        except sqlite3.Error as e:
            self.log_error(f"Error clearing embeddings: {str(e)}")
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á embedding: {e}")
            return

        print(f"\n‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {total_count:,} ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
        logging.info(f"Regenerating all {total_count} embeddings.")
        # Call _generate_missing_embeddings to re-process all texts (now marked as missing embeddings)
        self._generate_missing_embeddings(batch_size=batch_size) # Use provided batch_size
        # After regeneration, rebuild FAISS index
        self.rebuild_faiss_index()

    def _confirm_action(self, message: str) -> bool:
        """Helper function to ask for user confirmation for critical actions."""
        response = input(f"{message} [y/N]: ").strip().lower()
        return response == 'y'

    def _verify_system_ready(self):
        """Verifies that all core components of the RAG system are initialized and ready."""
        checks = [
            ("Embedding Model", hasattr(self, 'embedding_model') and self.embedding_model is not None),
            ("LLM", hasattr(self, 'llm') and self.llm is not None),
            ("Database Connection", hasattr(self, 'conn') and self.conn is not None),
            ("FAISS Index", self.faiss_index is not None and self.faiss_ids is not None)
        ]

        print("\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö:")
        logging.info("Verifying system readiness:")
        all_ready = True
        for name, status in checks:
            status_icon = "‚úÖ" if status else "‚ùå"
            print(f"{status_icon} {name}")
            logging.info(f"{name}: {'Ready' if status else 'Not Ready'}")
            if not status:
                all_ready = False

        if not all_ready:
            error_message = "System not ready. Please check the logs for details."
            logging.error(error_message)
            raise RuntimeError(error_message)

        print("\n‚ú® ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß!")
        logging.info("System is ready.")

    def add_texts_with_embeddings(self, texts: List[Dict]) -> Dict:
        """
        Adds new texts to the database, generates their embeddings, and updates the FAISS index.
        Each text's content is cleaned and validated before processing.
        """
        if not texts:
            error_message = "No data to add."
            logging.warning(error_message)
            return {'status': 'error', 'message': error_message}

        cursor = self.conn.cursor()
        stats = {
            'total': len(texts),
            'success': 0,
            'duplicates': 0,
            'invalid_content': 0, # New stat for invalid content
            'embedding_errors': 0, # New stat for embedding errors
            'db_errors': 0,      # New stat for database errors
            'details': []
        }
        
        batch_insert_data = [] # List to hold data for batch insertion

        # Use a context manager for SQLite transactions for automatic commit/rollback
        with self.conn:
            try:
                for text_entry in tqdm(texts, desc="Adding data with embeddings"):
                    try:
                        # Clean content and validate before processing
                        cleaned_content = self._clean_text(text_entry['content'])
                        if not self._is_valid_text(cleaned_content):
                            stats['invalid_content'] += 1
                            stats['details'].append(self._create_detail('invalid_content', text_entry, "Content invalid after cleaning."))
                            continue # Skip to next text_entry

                        # Check for duplicate using vol, page, item
                        if self._is_duplicate(cursor, text_entry):
                            stats['duplicates'] += 1
                            stats['details'].append(self._create_detail('duplicate', text_entry))
                            continue

                        # Generate embedding
                        # Ensure normalize_embeddings=True for content embeddings
                        embedding = self.embedding_model.encode([cleaned_content], convert_to_numpy=True, normalize_embeddings=True)[0]
                        if embedding.shape[0] != EMBEDDING_DIMENSION:
                            stats['embedding_errors'] += 1
                            stats['details'].append(self._create_detail('embedding_error', text_entry, f"Embedding dimension mismatch: {embedding.shape[0]}"))
                            continue # Skip if embedding dimension is wrong

                        embedding_bytes = embedding.astype(np.float32).tobytes()
                        current_timestamp = datetime.now().isoformat(timespec='seconds')

                        batch_insert_data.append((
                            text_entry['vol'],
                            text_entry['page'],
                            text_entry['item'],
                            cleaned_content, # Store the cleaned content
                            embedding_bytes,
                            current_timestamp, # created_at
                            current_timestamp  # updated_at
                        ))
                        stats['success'] += 1 # Increment success for valid data added to batch

                    except Exception as e:
                        # Catch any other errors during processing of a single text_entry
                        stats['db_errors'] += 1 # Count as a database-related error for this item
                        stats['details'].append(self._create_detail('error', text_entry, str(e)))
                
                # Perform batch insertion
                if batch_insert_data:
                    cursor.executemany("""
                        INSERT INTO texts (vol, page, item, content, embedding, created_at, updated_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, batch_insert_data)
                    # The transaction is committed automatically by 'with self.conn' if no exceptions occur
                    logging.info(f"Successfully added {len(batch_insert_data)} texts to the database.")

                print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {self.db_path}")
                self.rebuild_faiss_index() # Rebuild FAISS index after adding new data
                return {'status': 'success', 'stats': stats}

            except Exception as e:
                # Any exception caught here will cause the transaction to rollback
                self.log_error(f"Unexpected error during add_texts: {str(e)}")
                return {'status': 'error', 'message': str(e), 'stats': stats}

    def _is_duplicate(self, cursor, text: Dict) -> bool:
        """Checks if a text entry with the same vol, page, and item already exists in the database."""
        cursor.execute("""
            SELECT 1 FROM texts
            WHERE vol = ? AND page = ? AND item = ?
        """, (text['vol'], text['page'], text['item']))
        return cursor.fetchone() is not None

    def _create_detail(self, status: str, text: Dict, error: str = None) -> Dict:
        """Creates a standardized dictionary for logging details of text processing."""
        detail = {
            'status': status,
            'vol': text.get('vol'),
            'page': text.get('page'),
            'item': text.get('item'),
            'content_preview': text.get('content', '')[:100] # Add content preview for debugging
        }
        if error:
            detail['error'] = error
        return detail

    def retrieve_context(self, query: str, threshold: float = None) -> List[Dict]:
        """
        Retrieves relevant context (text snippets) from the database based on a query.
        Uses FAISS for similarity search and dynamic thresholding for relevance.
        Incorporates context expansion for richer context.
        """
        results = []

        # Validate FAISS index availability
        if not self.faiss_index or not isinstance(self.faiss_index, faiss.IndexIDMap) or self.faiss_ids is None:
            print("‚ö†Ô∏è FAISS index ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á rebuild index)")
            self.log_error("FAISS index is not ready or malformed.")
            return []

        index = self.faiss_index # The FAISS index object

        try:
            # Preprocess the query for consistency with stored embeddings
            query = self._preprocess_query(query)
            if not query:
                print("‚ö†Ô∏è ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ (‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•)")
                return []

            # Generate query embedding
            try:
                # Ensure query embedding is normalized if using L2 distance for cosine similarity
                query_embedding = self.embedding_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]
                query_embedding = np.array([query_embedding]).astype('float32') # Reshape for FAISS search
                
                if query_embedding.shape[1] != index.d:
                    print(f"‚ö†Ô∏è ‡∏°‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á Query Embedding ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏î‡∏±‡∏ä‡∏ô‡∏µ ({query_embedding.shape[1]} != {index.d})")
                    self.log_error(f"Query embedding dimension mismatch: {query_embedding.shape[1]} != {index.d}")
                    return []
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Query Embedding: {e}")
                self.log_error(f"Error creating query embedding: {e}")
                return []

            # Search in FAISS index for top K results
            try:
                distances, indices = index.search(query_embedding, TOP_K_RESULTS)
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô FAISS index: {e}")
                self.log_error(f"Error searching in FAISS index: {e}")
                return []

            # Determine dynamic threshold if not provided
            if threshold is None and len(distances[0]) > 0:
                min_d, avg_d, max_d = np.min(distances[0]), np.mean(distances[0]), np.max(distances[0])
                
                # --- IMPROVED DYNAMIC THRESHOLD CALCULATION ---
                # Goal: Make the threshold more lenient, allowing more relevant (but slightly further) results.
                # It should be closer to min_d, and not strictly capped by a low max_d * 0.8.
                
                # Option 1: A percentage increase from min_d (simple and effective)
                # This ensures the threshold is always above the best match.
                dynamic_threshold = min_d * 1.08 # Allow up to 8% increase from the best match distance

                # Apply reasonable bounds to prevent extreme values
                dynamic_threshold = max(dynamic_threshold, 0.8) # Ensure it's not too low (e.g., if min_d is very small)
                dynamic_threshold = min(dynamic_threshold, 1.8) # Cap the upper limit to prevent too much noise (adjust as needed)

                print(f"- ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: {min_d:.2f}")
                print(f"- ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_d:.2f}")
                print(f"- ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: {max_d:.2f}")
                print(f"- ‡πÉ‡∏ä‡πâ Dynamic Threshold (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß): {dynamic_threshold:.2f}")
            else:
                # Use provided threshold or a default fixed threshold
                dynamic_threshold = threshold if threshold is not None else 1.5 
                print(f"- ‡πÉ‡∏ä‡πâ Fixed Threshold: {dynamic_threshold:.2f}")

            # Filter results based on the determined threshold
            # Collect (id, distance) pairs for relevant chunks
            relevant_chunk_ids_with_distances = []
            for i, idx in enumerate(indices[0]):
                if distances[0][i] < dynamic_threshold:
                    relevant_chunk_ids_with_distances.append({'id': int(idx), 'distance': float(distances[0][i])})

            # Fallback: If no relevant results, return the single closest match
            if not relevant_chunk_ids_with_distances and len(indices[0]) > 0:
                closest_idx = int(indices[0][0])
                closest_distance = float(distances[0][0])
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ threshold. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (ID: {closest_idx}, Distance: {closest_distance:.2f})")
                try:
                    # Use _get_expanded_context_by_id for the single closest match as well
                    closest_match_data = self._get_expanded_context_by_id(closest_idx, closest_distance)
                    if closest_match_data:
                        self.last_closest_match = closest_match_data # Store for potential debugging
                        return closest_match_data # This returns a list of dicts
                    return []
                except Exception as e:
                    print(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {e}")
                    self.log_error(f"Error fetching closest match: {e}")
                    return []

            print(f"\nüîç ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: '{query}'")
            print(f"- ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö (‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î): {', '.join(f'{d:.2f}' for d in distances[0])}")
            print(f"- ‡πÉ‡∏ä‡πâ Threshold: {dynamic_threshold:.2f}")
            print(f"- ‡∏û‡∏ö {len(relevant_chunk_ids_with_distances)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏Å‡πà‡∏≠‡∏ô‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ö‡∏£‡∏¥‡∏ö‡∏ó)")

            # --- Context Expansion ---
            # Collect all unique IDs to fetch, including the relevant ones and their neighbors
            unique_ids_to_fetch = set()
            for chunk in relevant_chunk_ids_with_distances:
                unique_ids_to_fetch.add(chunk['id'])
                # Add neighbor IDs for context expansion
                # This assumes 'id' is somewhat sequential for adjacent chunks
                for i in range(1, CONTEXT_EXPANSION_WINDOW + 1):
                    unique_ids_to_fetch.add(chunk['id'] - i)
                    unique_ids_to_fetch.add(chunk['id'] + i)
            
            # Filter out invalid IDs (e.g., negative IDs)
            unique_ids_to_fetch = sorted([_id for _id in unique_ids_to_fetch if _id > 0])

            fetched_results = []
            if unique_ids_to_fetch:
                # Fetch all relevant texts and their neighbors from the database in a single query
                placeholders = ','.join('?' * len(unique_ids_to_fetch))
                sql_query = f"SELECT id, vol, page, item, content FROM texts WHERE id IN ({placeholders}) ORDER BY vol, page, item"
                try:
                    cursor = self.conn.cursor()
                    cursor.execute(sql_query, list(unique_ids_to_fetch))
                    rows = cursor.fetchall()

                    # Reconstruct results, associating distances to the primary relevant chunks
                    # And keeping the order for context
                    fetched_map = {row[0]: {'vol': row[1], 'page': row[2], 'item': row[3], 'content': row[4]} for row in rows}
                    
                    # Now, assemble the final list, including original distances for primary chunks
                    # and ensuring correct order based on vol, page, item
                    final_context_list = []
                    processed_ids = set() # To avoid adding same chunk multiple times if it's a neighbor of multiple relevant chunks

                    # Iterate through the original relevant chunks to ensure they are included
                    # and then add their neighbors
                    for original_chunk in relevant_chunk_ids_with_distances:
                        base_id = original_chunk['id']
                        base_distance = original_chunk['distance']

                        # Add the base chunk and its neighbors
                        for i in range(-CONTEXT_EXPANSION_WINDOW, CONTEXT_EXPANSION_WINDOW + 1):
                            current_id = base_id + i
                            if current_id in fetched_map and current_id not in processed_ids:
                                chunk_data = fetched_map[current_id].copy()
                                # Only assign distance if it's the original relevant chunk
                                if current_id == base_id:
                                    chunk_data['distance'] = base_distance
                                else:
                                    chunk_data['distance'] = -1 # Mark as neighbor, no direct distance
                                final_context_list.append(chunk_data)
                                processed_ids.add(current_id)
                    
                    # Sort the final list by vol, page, item to ensure logical flow
                    final_context_list.sort(key=lambda x: (x['vol'], x['page'], x['item']))

                    # --- Placeholder for Reranking ---
                    # If you implement a reranker (e.g., a cross-encoder model),
                    # you would apply it here to 'final_context_list'
                    # reranked_context_list = self._apply_reranking(query, final_context_list)
                    # return reranked_context_list
                    
                    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏´‡∏•‡∏±‡∏á‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ö‡∏£‡∏¥‡∏ö‡∏ó): {len(final_context_list)}")
                    print("\n[DEBUG] Retrieved results (with expanded context):")
                    for r in final_context_list:
                        dist_info = f" (Dist: {r['distance']:.2f})" if r['distance'] != -1 else " (Neighbor)"
                        print(f"- ‡πÄ‡∏•‡πà‡∏° {r['vol']} ‡∏´‡∏ô‡πâ‡∏≤ {r['page']} ‡∏Ç‡πâ‡∏≠ {r['item']} ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {r['content'][:100]}...{dist_info}")

                    return final_context_list

                except sqlite3.Error as db_e:
                    print(f"‚ö†Ô∏è Database error during context retrieval: {db_e}")
                    self.log_error(f"Database error during context retrieval: {db_e}")
                    return []

            print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(fetched_results)}")
            if not fetched_results:
                print("‚ö†Ô∏è ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á")
                return []

            # Sort results by distance (closest first) - this will be less relevant with context expansion
            # as neighbors have -1 distance. The sort by vol, page, item is more important now.
            # fetched_results.sort(key=lambda x: x['distance']) 
            
            return fetched_results

        except Exception as e:
            print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: {e}")
            self.log_error(f"General error during context retrieval: {e}")
            return []

    def _get_text_by_id(self, text_id: int, distance: float) -> Optional[Dict]:
        """
        Fetches a single text entry from the database by its ID.
        Uses the existing database connection.
        (This function is now less critical as retrieve_context handles batch fetching)
        """
        try:
            cursor = self.conn.cursor()
            sql_query = """
                SELECT vol, page, item, content FROM texts WHERE id = ?
            """
            logging.debug(f"Executing SQL query: {sql_query} with ID: {text_id}")
            cursor.execute(sql_query, (text_id,))
            row = cursor.fetchone()
            if row:
                text_data = {
                    'vol': row[0],
                    'page': row[1],
                    'item': row[2],
                    'content': row[3],
                    'distance': distance
                }
                logging.info(f"Found text in database: ID={text_id}, Data={text_data}")
                return text_data
            else:
                logging.warning(f"Text not found in database with ID: {text_id}")
                return None
        except sqlite3.Error as e:
            logging.error(f"Database error while fetching text with ID {text_id}: {str(e)}")
            return None
        except Exception as e:
            logging.error(f"Unexpected error while fetching text with ID {text_id}: {str(e)}")
            return None

    def _get_expanded_context_by_id(self, base_id: int, base_distance: float) -> List[Dict]:
        """
        Fetches the base chunk and its surrounding chunks (context window) from the database.
        Returns a list of dictionaries, sorted by vol, page, item.
        """
        ids_to_fetch = set()
        ids_to_fetch.add(base_id) # Always include the base chunk

        # Add neighbor IDs
        for i in range(1, CONTEXT_EXPANSION_WINDOW + 1):
            ids_to_fetch.add(base_id - i)
            ids_to_fetch.add(base_id + i)
        
        # Filter out invalid IDs (e.g., negative IDs) and sort for consistent query
        ids_to_fetch = sorted([_id for _id in ids_to_fetch if _id > 0])

        if not ids_to_fetch:
            return []

        placeholders = ','.join('?' * len(ids_to_fetch))
        sql_query = f"SELECT id, vol, page, item, content FROM texts WHERE id IN ({placeholders}) ORDER BY vol, page, item"
        
        try:
            cursor = self.conn.cursor()
            cursor.execute(sql_query, list(ids_to_fetch))
            rows = cursor.fetchall()

            expanded_context = []
            for row in rows:
                chunk_id = row[0]
                chunk_data = {
                    'vol': row[1],
                    'page': row[2],
                    'item': row[3],
                    'content': row[4],
                    'distance': base_distance if chunk_id == base_id else -1 # Mark neighbors with -1 distance
                }
                expanded_context.append(chunk_data)
            
            return expanded_context

        except sqlite3.Error as e:
            self.log_error(f"Database error during context expansion for ID {base_id}: {str(e)}")
            return []
        except Exception as e:
            self.log_error(f"Unexpected error during context expansion for ID {base_id}: {str(e)}")
            return []


    def generate_response(self, question: str) -> str:
        """
        Generates a natural language response to a question using retrieved context.
        The LLM prompt is refined to encourage more accurate and relevant responses.
        """
        context = self.retrieve_context(question)
        if not context:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å ‡πÇ‡∏õ‡∏£‡∏î‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"

        # Format the retrieved context for the LLM prompt
        # Limit context to prevent exceeding LLM's context window, prioritize closer matches
        max_context_tokens = 200 # Adjust based on LLM's n_ctx (4096) and expected response length
        current_context_tokens = 0
        selected_context_parts = []

        # It's better to sort by (vol, page, item) for logical flow, especially with context expansion
        # The retrieve_context already returns sorted by vol, page, item
        
        for c in context:
            # For context expansion, we might have neighbors with distance -1.
            # We can optionally prioritize primary relevant chunks or just include all.
            part_text = f"‡πÄ‡∏•‡πà‡∏° {c['vol']} ‡∏´‡∏ô‡πâ‡∏≤ {c['page']} ‡∏Ç‡πâ‡∏≠ {c['item']}:\n{c['content']}"
            # Estimate tokens (very rough, actual tokenization might differ)
            part_tokens = len(part_text.split()) # Word count as a proxy for token count
            
            if current_context_tokens + part_tokens < max_context_tokens:
                selected_context_parts.append(part_text)
                current_context_tokens += part_tokens
            else:
                break # Stop adding context if it exceeds max tokens

        context_text = "\n\n".join(selected_context_parts) # Use double newline for better separation

        # Refined LLM Prompt
        prompt = f"""
            ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á
            ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å" ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å"

            ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å:
            {context_text}

            ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

            ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
        """

        try:
            output = self.llm(
                prompt,
                max_tokens=4096, # Max tokens for the LLM's response
                stop=["\n\n", "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å:"], # Stop generation at these phrases
                temperature=0.2, # Lower temperature for more factual, less creative responses
                top_k=40,        # Top-k sampling
                top_p=0.95,      # Top-p (nucleus) sampling
                repeat_penalty=1.1 # Penalty for repeating tokens
            )
            if 'choices' not in output or len(output['choices']) == 0:
                raise ValueError("Invalid model output structure: 'choices' key missing or empty.")
            
            response_text = output['choices'][0]['text'].strip()
            # Post-processing to handle cases where LLM might still generate unwanted parts
            if "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å" in response_text:
                return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å ‡πÇ‡∏õ‡∏£‡∏î‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"
            
            return response_text
        except Exception as e:
            self.log_error(f"LLM generation failed: {str(e)}")
            print(f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {e}") 
            self.log_error(f"LLM generation failed: {str(e)} - Details: {repr(e)}")
            return "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"

    def main(self):
        """Main interaction loop for the RAG system."""
        print("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏û‡∏£‡∏∞‡πÑ‡∏ï‡∏£‡∏õ‡∏¥‡∏é‡∏Å")
        # Ensure the system is fully ready before starting the interactive loop
        try:
            self._verify_system_ready()
        except RuntimeError as e:
            print(f"‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {e}")
            return # Exit if system is not ready

        while True:
            try:
                question = input("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¥‡∏°‡∏û‡πå '‡∏≠‡∏≠‡∏Å'): ").strip()
                if question.lower() == '‡∏≠‡∏≠‡∏Å':
                    break
                # Basic input validation to prevent injection or malformed queries
                if any(char in question for char in ['{', '}', '=', ':']) or 'import' in question:
                    print("‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï)")
                    continue
                if not question:
                    print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡πâ‡∏≠‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
                    continue

                response = self.generate_response(question)
                print("\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:")
                print(response)
            except Exception as e:
                self.log_error(f"Main loop error: {str(e)}")
                print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

    def _cleanup(self):
        """Cleans up resources (closes database connection, potentially releases LLM resources)."""
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
            logging.info("Database connection closed.")
        if hasattr(self, 'llm') and self.llm:
            # llama_cpp.Llama objects do not have an explicit .close() method.
            # Python's garbage collection should handle resource release.
            logging.info("LLM resources potentially released (no explicit close method).")
        end_time = datetime.now()
        duration = end_time - self.start_time
        logging.info(f"System ended at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}, Total runtime: {duration}")
        print(f"\n‚ú® ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ({end_time.strftime('%Y-%m-%d %H:%M:%S')})")

# Standalone test function (kept outside the class for direct testing)
def test_get_text_by_id(db_path: str, text_id: int):
    """Simple test function to fetch text by ID from a given database path."""
    try:
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT vol, page, item, content FROM texts WHERE id = ?", (text_id,))
            row = cursor.fetchone()
            if row:
                print(f"Found text with ID {text_id}: {row}")
            else:
                print(f"Text with ID {text_id} not found")
    except sqlite3.Error as e:
        print(f"Database error: {str(e)}")
    except Exception as e:
        print(f"Unexpected error: {str(e)}")

if __name__ == "__main__":
    # To run this, ensure you have a 'rag_data.sqlite' with a 'texts' table
    # populated with cleaned content and embeddings (e.g., by running migrate_data.py first).
    rag_system = BuddhistRAGSystem()
    try:
        # Optional: Force regenerate all embeddings if needed for a fresh start
        #rag_system._regenerate_all_embeddings() 
        rag_system.main()
    finally:
        # Ensure cleanup happens even if an error occurs in the main loop
        rag_system._cleanup()
